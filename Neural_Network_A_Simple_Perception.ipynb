{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is deep learning, and how is it connected to artificial intelligence\n",
        "\n",
        "\n",
        "Deep learning is a type of machine learning that uses artificial neural networks with multiple layers to analyze data and make predictions or decisions. It is a subset of machine learning, which itself is a subset of artificial intelligence (AI). Essentially, deep learning is a more advanced and complex form of machine learning that leverages neural networks to tackle intricate problems and achieve human-like intelligence in specific tasks.\n",
        "Here's a breakdown of the relationship:\n",
        "Artificial Intelligence (AI):\n",
        "The broadest term, encompassing the concept of creating machines that can perform tasks that typically require human intelligence.\n",
        "Machine Learning (ML):\n",
        "A subset of AI that focuses on algorithms that allow machines to learn from data without explicit programming.\n",
        "Deep Learning (DL):\n",
        "A specialized subset of machine learning that utilizes deep neural networks (neural networks with multiple layers) to analyze data and learn complex patterns.\n",
        "AI vs. Machine Learning (2025): Key Differences | Built In\n",
        "In essence: Deep learning is a powerful tool within the broader field of AI, enabling machines to learn from vast amounts of data and perform tasks that were once considered exclusively within the realm of human intelligence.\n",
        "\n",
        "2. What is a neural network, and what are the different types of neural networks.\n",
        "\n",
        "\n",
        "A neural network is a computational model, inspired by the human brain, that learns to perform tasks by analyzing examples. It's a type of machine learning algorithm that uses interconnected nodes, or neurons, organized in layers to process data. Different types of neural networks are designed for specific tasks and data structures.\n",
        "\n",
        "3.  What is the mathematical structure of a neural network.\n",
        "\n",
        "\n",
        "A neural network's mathematical structure involves nodes (neurons), connections (weights), and layers, with calculations based on linear algebra and calculus. Each node performs a calculation involving weighted inputs, a bias, and an activation function.\n",
        "Here's a breakdown:\n",
        "* . Nodes (Neurons):\n",
        "Each node represents a computational unit.\n",
        "It receives input signals from other nodes or from the input layer.\n",
        "These inputs are multiplied by corresponding weights, representing the strength of the connection.\n",
        "A bias term is added to the weighted sum.\n",
        "The result is then passed through an activation function.\n",
        "* . Connections (Weights):\n",
        "Connections represent the strength of the relationship between neurons.\n",
        "Each connection has an associated weight that determines how much influence one neuron has on another.\n",
        "These weights are learned during the training process.\n",
        "* . Layers:\n",
        "Neurons are organized into layers: input, hidden, and output.\n",
        "The input layer receives the initial data.\n",
        "Hidden layers perform intermediate computations.\n",
        "The output layer produces the final result.\n",
        "\n",
        "4. What is an activation function, and why is it essential in neural network.\n",
        "\n",
        "An activation function in a neural network is a mathematical function that introduces non-linearity to the network's output, allowing it to learn complex patterns and relationships in data. Without it, a neural network would only be capable of performing linear transformations, limiting its ability to solve complex problems.\n",
        "\n",
        "5. Could you list some common activation functions used in neural networks.\n",
        "\n",
        "Commonly used activation functions in neural networks include Sigmoid, Tanh, ReLU, Leaky ReLU, and Softmax. Each function has unique properties that affect the network's learning process and performance.\n",
        "\n",
        "6. What is a multilayer neural network.\n",
        "\n",
        "A multilayer neural network, also known as a multilayer perceptron (MLP), is a feedforward artificial neural network composed of multiple layers of interconnected nodes or neurons. It consists of an input layer, one or more hidden layers, and an output layer. Each layer performs computations on the data, with connections between layers allowing for complex pattern recognition and data processing.\n",
        "\n",
        "7. What is a loss function, and why is it crucial for neural network training.\n",
        "\n",
        "A loss function quantifies the difference between a neural network's predicted output and the actual target value, essentially measuring the error of the network's predictions. It's crucial for training because it provides a way to optimize the network by adjusting its internal parameters (weights and biases) to minimize this error. By using the loss function in conjunction with optimization algorithms like gradient descent, the network learns to make more accurate predictions over time.\n",
        "\n",
        "8. What are some common types of loss functions.\n",
        "\n",
        "Common loss functions in machine learning include Mean Squared Error (MSE), Mean Absolute Error (MAE), Huber Loss, Cross-Entropy Loss (including Binary Cross-Entropy and Categorical Cross-Entropy), and Hinge Loss. These loss functions are used to quantify the difference between predicted and actual values, guiding the training process of machine learning models.\n",
        "\n",
        "9. How does a neural network learn.\n",
        "\n",
        "Neural networks learn through a process of adjusting their internal connections, called weights, based on training data. This learning process involves feeding the network a dataset, generating an output, and then refining the weights to minimize the difference between the predicted output and the actual (correct) output. This iterative process continues until the network achieves a desired level of accuracy.\n",
        "\n",
        "\n",
        "10. What is an optimizer in neural networks, and why is it necessary.\n",
        "\n",
        "\n",
        "In neural networks, an optimizer is an algorithm that adjusts the network's weights to minimize the loss function, which measures the difference between the network's predictions and the actual values. Optimizers are crucial because they enable the neural network to learn from data and improve its performance over time by iteratively updating the weights in the right direction.\n",
        "\n",
        "\n",
        "11.  Could you briefly describe some common optimizers.\n",
        "\n",
        "Common optimizers include Stochastic Gradient Descent (SGD), Adam, and RMSprop, each employing specific update rules, learning rates, and momentum for refining model parameters.\n",
        "\n",
        "12. Can you explain forward and backward propagation in a neural network.\n",
        "\n",
        "In a neural network, forward and backward propagation are two fundamental processes. Forward propagation is the process of feeding input data through the network to produce an output, while backward propagation is the process of adjusting the network's weights and biases based on the error between the predicted output and the actual output.\n",
        "\n",
        "13. What is weight initialization, and how does it impact training.\n",
        "\n",
        "How to Initialize Weights in Neural Networks?Weight initialization is the process of assigning starting values to the weights in a neural network before training begins. It significantly impacts training by influencing how quickly the network learns and whether it converges to a good solution. Poorly chosen initial weights can lead to slow training, vanishing or exploding gradients, and ultimately, suboptimal performance.\n",
        "\n",
        "14. What is the vanishing gradient problem in deep learning.\n",
        "\n",
        "The vanishing gradient problem in deep learning refers to the phenomenon where gradients, used to update neural network weights during training, become extremely small as they are propagated backward through the network. This leads to minimal updates in the earlier layers, effectively hindering learning in those parts of the network and potentially slowing down or halting the overall training process.\n",
        "\n",
        "15. What is the exploding gradient problem?\n",
        "\n",
        "The exploding gradient problem in neural networks occurs when gradients, used to update model weights during training, become excessively large. This leads to unstable training, with the model's loss function potentially increasing instead of decreasing, and can even cause the training process to diverge. Essentially, the model's weights are updated by very large amounts, making it difficult for the network to learn effectively.\n",
        "\n",
        "                        \n",
        "                          ###  Practical\n",
        "\n",
        "1. How do you create a simple perceptron for basic binary classification.\n",
        "\n",
        "   A simple perceptron for binary classification is a single-layer neural network that learns to classify data into two categories. It works by taking weighted inputs, summing them, and applying an activation function (like a step function) to produce a binary output (0 or 1). The perceptron learns by adjusting these weights based on the errors in its predictions.\n",
        "\n",
        "\n",
        "2. How can you build a neural network with one hidden layer using Keras.\n",
        "\n",
        "    To create a neural network with one hidden layer using Keras, you'll use the Sequential model and add Dense layers. The Dense layer represents a fully connected layer, and you'll specify the number of neurons and activation function. Here's a code example:"
      ],
      "metadata": {
        "id": "jdoIh8X-CwJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Input(shape=(input_size,)),  # Input layer (specify input size)\n",
        "    Dense(hidden_units, activation='relu'),  # Hidden layer\n",
        "    Dense(output_size, activation='sigmoid')  # Output layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "7HwUuivIFO36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.   How do you initialize weights using the Xavier (Glorot) initialization method in Keras.\n",
        "\n",
        "    The Glorot uniform initializer, also called Xavier uniform initializer. Draws samples from a uniform distribution within [-limit, limit] , where limit = sqrt(6 / (fan_in + fan_out)) ( fan_in is the number of input units in the weight tensor and fan_out is the number of output units).\n",
        "\n",
        "4.  How can you apply different activation functions in a neural network in Keras.\n",
        "\n",
        "      In Keras, you can apply different activation functions to neural network layers in two primary ways: by using the activation parameter in a layer's constructor or by explicitly adding an Activation layer. Built-in activation functions can be referenced by name (e.g., 'relu', 'sigmoid', 'tanh'), or you can define and use your own custom activation function.\n",
        "      *. Using the activation parameter:\n",
        "       This is the most common and convenient way to apply activation functions. It's done by passing the desired activation function (as a string or a callable) as the activation argument to the layer constructor."
      ],
      "metadata": {
        "id": "y8Uqfa1wGiAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(784,)),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "v2Vw7g46HGxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do you add dropout to a neural network model to prevent overfitting.\n",
        "\n",
        "    Dropout is a regularization technique used in neural networks to prevent overfitting by randomly dropping out (setting to zero) a fraction of neurons during each training iteration. This forces the network to learn more robust and generalized features by preventing it from relying too heavily on any single neuron.\n",
        "\n",
        "6. How do you manually implement forward propagation in a simple neural network.\n",
        "\n",
        "    Forward propagation in a simple neural network involves passing input data through the network's layers, computing weighted sums at each neuron, applying activation functions, and ultimately producing an output. This process moves information forward from the input layer to the output layer, without any feedback loops.\n",
        "\n",
        "7. How do you add batch normalization to a neural network model in Keras.\n",
        "\n",
        "\n",
        "    Syntax of BatchNormalization Class in Keras:\n",
        "    tf.keras.layers.BatchNormalization( axis=-1,\n",
        "    momentum=0.99, epsilon=0.001,\n",
        "    center=True, scale=True,\n",
        "    beta_initializer=\"zeros\", gamma_initializer=\"ones\",\n",
        "    beta_regularizer=None, gamma_regularizer=None,\n",
        "    beta_constraint=None, gamma_constraint=None,\n",
        "\n",
        "\n",
        "8.  How can you visualize the training process with accuracy and loss curves.\n",
        "\n",
        "    To visualize the training process of a machine learning model, you can plot accuracy and loss curves. These curves show how the model's performance changes over the course of training. Accuracy curves typically show the percentage of correct predictions, while loss curves represent the error rate. Visualizing these curves helps in understanding model behavior, identifying potential problems like overfitting or underfitting, and optimizing the training process.\n",
        "\n",
        "\n",
        "9. How can you use gradient clipping in Keras to control the gradient size and prevent exploding gradients.\n",
        "\n",
        "\n",
        "    Gradient Clipping: Gradient clipping involves imposing a threshold on the gradients during backpropagation. Limit the magnitude of gradients during backpropagation, this can prevent them from becoming too small or exploding, which can also hinder learning.\n",
        "\n",
        "10.  How can you create a custom loss function in Keras.\n",
        "\n",
        "    To create a custom loss function in Keras, define a Python function that takes the true labels (y_true) and predicted labels (y_pred) as input, and returns a single tensor representing the loss value. This function can then be passed to the compile() method of your Keras model.\n",
        "\n",
        "11. How can you visualize the structure of a neural network model in Keras?\n",
        "\n",
        "    You can visualize a Keras neural network model using the plot_model function or third-party libraries like visualkeras or Netron. The plot_model function, built into Keras, generates a visual representation of the model's architecture, showing layers and connections. visualkeras offers more customization options and can handle various model types, including CNNs. Netron provides a detailed view of individual layers and their attributes.\n"
      ],
      "metadata": {
        "id": "QLCIHiEBHL0D"
      }
    }
  ]
}