{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is TensorFlow 2.0, and how is it different from TensorFlow 1.x2\n",
        "\n",
        "  TensorFlow 2.0 is a major upgrade focused on ease of use and productivity compared to TensorFlow 1.x. Key changes include the introduction of eager execution, a more intuitive Keras API, and simplified model building. TensorFlow 2.0 also includes a more robust automatic differentiation system and removes or redesigns redundant APIs.\n",
        "\n",
        "2. How do you install TensorFlow 2.02\n",
        "\n",
        "  To install TensorFlow 2.0.2, first ensure you have Python 3.6-3.8 installed. Then, use pip to install TensorFlow within a virtual environment. If you plan to use GPU acceleration, you'll need to install CUDA and cuDNN separately, and then install the tensorflow-gpu package. Finally, verify your installation by running a simple TensorFlow script.\n",
        "\n",
        "3.  What is the primary function of the tf.function in TensorFlow 2.02\n",
        "\n",
        "  You can use tf. function to make graphs out of your programs. It is a transformation tool that creates Python-independent dataflow graphs out of your Python code. This will help you create performant and portable models, and it is required to use SavedModel\n",
        "\n",
        "4.  What is the purpose of the Model class in TensorFlow 2.02\n",
        "\n",
        "  The Model class in Keras offers a more flexible and powerful approach for constructing neural networks. It allows you to define complex models with multiple inputs and outputs, as well as implement shared layers or model architectures with branching and merging capabilities\n",
        "\n",
        "5. How do you create a neural network using TensorFlow 2.02\n",
        "\n",
        "  To create a neural network using TensorFlow 2.0, you can use the Keras API, which is integrated into TensorFlow. The basic steps involve importing necessary libraries, loading and preprocessing data, defining the model architecture, compiling the model, training it, and evaluating its performance.\n",
        "\n",
        "6. What is the importance of Tensor Space in TensorFlow2\n",
        "\n",
        "  Tensors are multi-dimensional arrays that can hold data of different types. In TensorFlow, tensors are the primary building blocks for creating computational graphs and performing operations in machine learning models. Tensors can be of various ranks, including scalars, vectors, matrices, and higher-dimensional arrays.2\n",
        "\n",
        "7. How can TensorBoard be integrated with TensorFlow 2.02\n",
        "\n",
        "    Setup\n",
        "    # Load the TensorBoard notebook extension %load_ext tensorboard.\n",
        "    import tensorflow as tf import datetime, os.\n",
        "    fashion_mnist = tf. datasets. fashion_mnist (x_train, y_train),(x_test, y_test) = fashion_mnist. ...\n",
        "    %tensorboard --logdir logs.\n",
        "    %tensorboard --logdir logs\n",
        "\n",
        "8. What is the purpose of TensorFlow Playground2\n",
        "\n",
        "  TensorFlow Playground 2, also simply called TensorFlow Playground, is an interactive web-based tool for visualizing and experimenting with neural networks without needing to write any code. It allows users to explore the fundamentals of machine learning and neural networks by adjusting various parameters like learning rate, activation functions, and regularization to observe their effects on model training.\n",
        "\n",
        "9. What is Netron, and how is it useful for deep learning models2\n",
        "\n",
        "  n deep learning, a neural network is a complex web of interconnected nodes, or \"neurons,\" organized into layers. These layers work together to process and transform data, learning to recognize patterns and make predictions or decisions based on the input they receive.\n",
        "\n",
        "10. What is the difference between TensorFlow and PyTorch2\n",
        "\n",
        "  TensorFlow and PyTorch are both popular deep learning frameworks, but they differ in their strengths and approaches. TensorFlow is generally favored for production environments and large-scale deployments due to its scalability and deployment options. PyTorch, on the other hand, is often preferred for research and rapid prototyping because of its flexibility, dynamic computation graphs, and ease of use, especially for beginners.\n",
        "\n",
        "11. How do you install PyTorch2\n",
        "\n",
        "  To install PyTorch 2, you need to navigate to the PyTorch website and select your desired configuration, including your operating system, package manager (pip or conda), language (Python), and compute platform (CPU or GPU). Then, copy the installation command and run it in your terminal.\n",
        "\n",
        "12. What is the basic structure of a PyTorch neural network2\n",
        "\n",
        "  Input layer: It contains n neurons such that you have n features which will be used as predictors. Hidden layer: Hidden layers can contain n-hidden layers. It depends how complex structure you want to build. Output Layer: Output layer contains n neurons according to the n-classes or the type of problem we are solving\n",
        "\n",
        "13. What is the significance of tensors in PyTorch2\n",
        "\n",
        "  Tensors are the core data structure in PyTorch and many other deep learning frameworks. They are like arrays (n-dimensional arrays) and matrices (2-dimensional tensors), but unlike arrays and matrices, tensors can be used on hardware accelerators like GPUs.\n",
        "\n",
        "14. What is the difference between torch.Tensor and torch.cuda.Tensor in PyTorch2\n",
        "\n",
        "  CUDA cores execute operations sequentially in parallel, meaning multiple cores perform calculations simultaneously but at a more granular level. Tensor cores leverage fused multiply-add (FMA) operations to perform multiple calculations in a single clock cycle.\n",
        "\n",
        "15. What is the purpose of the torch.optim module in PyTorch2\n",
        "\n",
        "  The torch.optim module in PyTorch provides implementations of various optimization algorithms that are crucial for training neural networks. It allows users to update model parameters (weights and biases) during the training process to minimize the loss function and improve model performance. Essentially, it's the engine that drives the learning process by adjusting the model's parameters based on calculated gradients.\n",
        "\n",
        "16. What are some common activation functions used in neural networks2\n",
        "\n",
        "  ommonly used activation functions in neural networks include Sigmoid, Tanh, ReLU, Leaky ReLU, and Softmax. These functions introduce non-linearity, allowing networks to learn complex patterns. ReLU is particularly popular for hidden layers, while Softmax is often used in the output layer for multi-class classification.\n",
        "\n",
        "17. What is the difference between torch.nn.Module and torch.nn.Sequential in PyTorch2\n",
        "\n",
        "  Sequential is a container that is used to hold the building blocks of a neural network \"sequentially\". The difference - ➡ The nn. Module's stored in Sequential are connected in a cascaded way. Hence the output of the 1st Module stored in Sequential becomes the input to the 2nd Module -- need to take care of dimensions.\n",
        "\n",
        "18.  How can you monitor training progress in TensorFlow 2.02\n",
        "\n",
        "  Monitoring training progress in TensorFlow 2.x, especially with Keras, can be achieved through several methods:\n",
        "  1. Built-in Keras fit() output:\n",
        "  When using model.fit(), Keras automatically displays a progress bar and prints out metrics like loss and accuracy for each epoch, including validation metrics if a validation_data or validation_split is provided\n",
        "\n",
        "19.  How does the Keras API fit into TensorFlow 2.02\n",
        "\n",
        "  TensorFlow 2.0 comes with Keras packaged inside, there is no need to import Keras as a separate module (although you can do this if you need). TensorFlow 2.0 API is simplified and improved. This is good news for us — Machine Learning developers. I'm using tf.\n",
        "\n",
        "20. What is an example of a deep learning project that can be implemented using TensorFlow 2.02\n",
        "\n",
        "  Summary. In this blog, we explored the powerful capabilities of deep learning with TensorFlow through four key projects: Image Classification: Building a CNN for classifying images using the CIFAR-10 dataset. Object Detection: Implementing YOLO for detecting objects in images with the COCO dataset.\n",
        "\n",
        "21. What is the main advantage of using pre-trained models in TensorFlow and PyTorch?\n",
        "\n",
        "  The main advantage of using pre-trained models in TensorFlow and PyTorch is transfer learning, which significantly reduces the time, computational resources, and data required to train a new model for a specific task.\n",
        "  Pre-trained models have already learned a vast amount of features and patterns from large datasets, such as ImageNet for computer vision or massive text corpora for natural language processing. By leveraging these pre-trained models, one can:\n",
        "\n",
        "  Save Training Time and Resources:\n",
        "\n",
        "  Instead of training a model from scratch, which can take days or weeks on powerful hardware, pre-trained models can be fine-tuned on a smaller, task-specific dataset in a much shorter timeframe and with fewer computational resources.\n",
        "\n",
        "  Improve Performance with Limited Data:\n",
        "\n",
        "  When a specific task has limited training data, training a model from scratch often leads to overfitting and poor generalization. Pre-trained models provide a strong starting point, allowing them to generalize better even with smaller datasets.\n",
        "\n",
        "  Benefit from Learned Features:\n",
        "  \n",
        "  The pre-trained model's learned features are often highly generalizable and effective for various related tasks. This allows for the \"transfer\" of knowledge from a broad domain to a more specific one."
      ],
      "metadata": {
        "id": "Emymq5TJKUhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "                                                   ### Practical"
      ],
      "metadata": {
        "id": "6HbHIOMKNbil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  How do you install and verify that TensorFlow 2.0 was installed successfully2\n",
        "\n",
        "    Step-by-step instructions\n",
        "    System requirements. Ubuntu 16.04 or higher (64-bit) ...\n",
        "    GPU setup. You can skip this section if you only run TensorFlow on the CPU. ...\n",
        "    Create a virtual environment with venv. ...\n",
        "    Install TensorFlow. ...\n",
        "    Verify the installation. ...\n",
        "    [GPU only] Virtual environment configuration. ...\n",
        "    System requirements. ...\n",
        "    Check Python version.\n",
        "\n",
        "2.  How can you define a simple function in TensorFlow 2.0 to perform addition2\n",
        "\n",
        "  In TensorFlow 2, eager execution is turned on by default. The user interface is intuitive and flexible (running one-off operations is much easier and faster), but this can come at the expense of performance and deployability.\n",
        "\n",
        "  You can use tf.function to make graphs out of your programs. It is a transformation tool that creates Python-independent dataflow graphs out of your Python code. This will help you create performant and portable models, and it is required to use SavedModel.\n",
        "\n",
        "  This guide will help you conceptualize how tf.function works under the hood, so you can use it effectively.\n",
        "\n",
        "  The main takeaways and recommendations are:\n",
        "\n",
        "  Debug in eager mode, then decorate with @tf.function.\n",
        "  Don't rely on Python side effects like object mutation or list appends.\n",
        "  tf.function works best with TensorFlow ops; NumPy and Python calls are converted to\n",
        "\n",
        "3.  How can you create a simple neural network in TensorFlow 2.0 with one hidden layer2\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "lZ3M72BENhzM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c1f154c"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(784,)), # Hidden layer with 64 neurons and ReLU activation\n",
        "    Dense(10, activation='softmax') # Output layer with 10 neurons and Softmax activation for classification\n",
        "])\n",
        "\n",
        "# Compile the model (example using Adam optimizer and sparse categorical crossentropy loss)\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. ow can you visualize the training progress using TensorFlow and Matplotlib2"
      ],
      "metadata": {
        "id": "6-QqfNOQOefI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07ab503e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Assuming you have trained a model and stored the history\n",
        "# history = model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val)) # Example history\n",
        "\n",
        "# Create some dummy history data for demonstration purposes\n",
        "history = tf.keras.callbacks.History()\n",
        "history.history['loss'] = [1.0, 0.8, 0.6, 0.4, 0.2]\n",
        "history.history['accuracy'] = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "history.history['val_loss'] = [1.1, 0.9, 0.7, 0.5, 0.3]\n",
        "history.history['val_accuracy'] = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  How do you install PyTorch and verify the PyTorch installation2\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "38zP98hwOxpH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "496e68d7"
      },
      "source": [
        "  To install PyTorch and verify your installation, follow these steps:\n",
        "\n",
        "1.  **Visit the PyTorch Website:** Go to the official PyTorch website ([https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)).\n",
        "2.  **Select Your Configuration:** Choose the appropriate options for your system (your operating system, package manager like pip or conda, Python version, and CUDA version if you have a GPU).\n",
        "3.  **Copy the Installation Command:** The website will generate the exact installation command based on your selections. Copy this command.\n",
        "4.  **Run the Installation Command:** Open your terminal or command prompt and paste the copied command. Press Enter to start the installation.\n",
        "5.  **Verify the Installation:** To verify that PyTorch was installed correctly, open a Python interpreter or a new code cell in your notebook and run the following code:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  How do you create a simple neural network in PyTorch2"
      ],
      "metadata": {
        "id": "kFN-OfOoPB1N"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "461eacb3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a simple neural network class\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size) # First fully connected layer\n",
        "        self.relu = nn.ReLU() # ReLU activation function\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes) # Second fully connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Example usage:\n",
        "input_size = 784  # Example input size (e.g., for flattened MNIST image)\n",
        "hidden_size = 128 # Example hidden layer size\n",
        "num_classes = 10  # Example number of output classes (e.g., for MNIST)\n",
        "\n",
        "# Create an instance of the model\n",
        "model = SimpleNN(input_size, hidden_size, num_classes)\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  How do you define a loss function and optimizer in PyTorch2"
      ],
      "metadata": {
        "id": "XoI9i82ZPMk9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c18c86be"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Assuming you have a model defined (e.g., the SimpleNN from the previous example)\n",
        "# model = SimpleNN(input_size, hidden_size, num_classes)\n",
        "\n",
        "# Define a loss function (e.g., Cross-Entropy Loss for classification)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define an optimizer (e.g., Adam optimizer)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) # You can adjust the learning rate (lr)\n",
        "\n",
        "# Now you can use criterion and optimizer during your training loop\n",
        "# For example:\n",
        "# outputs = model(inputs)\n",
        "# loss = criterion(outputs, labels)\n",
        "# loss.backward() # Backpropagation\n",
        "# optimizer.step() # Update model parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. How do you implement a custom loss function in PyTorch2"
      ],
      "metadata": {
        "id": "AU9nxgEKPVL7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bcd3791"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomMSELoss, self).__init__()\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        # Calculate the mean squared error\n",
        "        loss = torch.mean((outputs - targets)**2)\n",
        "        return loss\n",
        "\n",
        "# Example usage:\n",
        "# Assuming you have model outputs and corresponding target values\n",
        "# outputs = torch.randn(10, 1) # Example model outputs\n",
        "# targets = torch.randn(10, 1) # Example target values\n",
        "\n",
        "# Create an instance of the custom loss function\n",
        "# custom_criterion = CustomMSELoss()\n",
        "\n",
        "# Calculate the loss\n",
        "# loss = custom_criterion(outputs, targets)\n",
        "# print(f\"Custom MSE Loss: {loss.item()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. How do you save and load a TensorFlow model?"
      ],
      "metadata": {
        "id": "Lsu4p_QWPcHn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5965d9e6"
      },
      "source": [
        "### Saving a TensorFlow Model (SavedModel format)\n",
        "\n",
        "The SavedModel format is a language-neutral, recoverable serialization format for TensorFlow models. It allows you to save the trained model and use it later for inference or further training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69e7814b"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import os\n",
        "\n",
        "# Assume you have a trained model (e.g., the one from the previous examples)\n",
        "# For demonstration, let's create a simple model and save it\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(784,)),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Create a directory to save the model\n",
        "model_dir = './my_saved_model'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "# Save the model in the SavedModel format\n",
        "model.save(model_dir)\n",
        "\n",
        "print(f\"Model saved to {model_dir}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74ba262f"
      },
      "source": [
        "### Loading a TensorFlow Model (SavedModel format)\n",
        "\n",
        "Once you have saved a model, you can load it back using `tf.keras.models.load_model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ad713ac"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Specify the directory where the model was saved\n",
        "model_dir = './my_saved_model'\n",
        "\n",
        "# Load the model\n",
        "loaded_model = tf.keras.models.load_model(model_dir)\n",
        "\n",
        "# Print the model summary to verify it's loaded correctly\n",
        "loaded_model.summary()\n",
        "\n",
        "# You can now use the loaded_model for prediction or further training\n",
        "# For example:\n",
        "# predictions = loaded_model.predict(new_data)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}