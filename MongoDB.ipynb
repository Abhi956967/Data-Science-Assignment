{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "                                              #### Theoretical Questions"
      ],
      "metadata": {
        "id": "Wdo_QfRwZb_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  What are the key differences between SQL and NoSQL databases\n",
        "\n",
        "  SQL and NoSQL databases differ primarily in their structure, scalability, and data models. SQL databases are relational, using a structured query language and typically scaling vertically. NoSQL databases are non-relational, employing various data models (document, key-value, etc.) and scaling horizontally.\n",
        "\n",
        "2. What makes MongoDB a good choice for modern applications\u001d\n",
        "\n",
        "\n",
        "  MongoDB is a good choice for modern applications due to its flexibility, scalability, and performance characteristics. Its document-oriented data model and ability to handle unstructured data make it well-suited for agile development and evolving business needs.\n",
        "\n",
        "3. Explain the concept of collections in MongoDB<\n",
        "\n",
        "  MongoDB - Database, Collection, and Document - GeeksforGeeksIn MongoDB, a collection is a grouping of documents, similar to a table in a relational database. It's a fundamental structure within a database that stores and organizes data as a set of documents. Each document within a collection can have a different structure, offering flexibility in how data is stored.\n",
        "\n",
        "\n",
        "4. < How does MongoDB ensure high availability using replication\n",
        "\n",
        "  MongoDB ensures high availability through replica sets, which are groups of mongod instances that maintain the same data set. This redundancy allows for automatic failover, meaning that if the primary node fails, a secondary node is elected to take over, minimizing downtime and ensuring continuous operation.\n",
        "\n",
        "5. What are the main benefits of MongoDB Atlas\u001d\n",
        "\n",
        "  MongoDB Atlas offers several key benefits, making it a popular choice for modern application development. These include ease of use, scalability, security, and automated management. It simplifies database deployment and maintenance, allowing developers to focus on building applications rather than managing infrastructure.\n",
        "\n",
        "6. What is the role of indexes in MongoDB, and how do they improve performance\u001d\n",
        "\n",
        "  In MongoDB, indexes enhance query performance by allowing the database to quickly locate and retrieve data, rather than scanning every document in a collection. This is achieved by creating an ordered data structure that acts as a roadmap, enabling MongoDB to jump directly to relevant documents that match query criteria. Indexes are crucial for efficient data retrieval, especially in large collections, and can significantly reduce query execution time and resource consumption\n",
        "\n",
        "\n",
        "7. Describe the stages of the MongoDB aggregation pipeline<\n",
        "\n",
        "  MongoDB Aggregation: tutorial with examples and exercises ...The MongoDB aggregation pipeline processes documents through a series of stages, each performing a specific operation, transforming the data along the way. The output of one stage becomes the input for the next, creating a flow of data processing. Key stages include $match to filter documents, $group to aggregate data, and $sort to order results.\n",
        "  \n",
        "8. What is sharding in MongoDB? How does it differ from replication\u001d\n",
        "\n",
        "  In MongoDB, sharding and replication serve different purposes in scaling and data management. Sharding distributes data across multiple servers (shards) to handle large datasets and high throughput, effectively scaling horizontally. Replication, on the other hand, creates copies of the entire dataset on multiple servers, primarily for high availability and read scalability.\n",
        "\n",
        "9.  What is PyMongo, and why is it used\u001d\n",
        "\n",
        "  In MongoDB, ACID properties (Atomicity, Consistency, Isolation, and Durability) ensure the reliability and integrity of database transactions. These properties guarantee that a transaction is treated as a single unit of work, either fully completing or having no effect, and that the database remains in a valid state throughout.\n",
        "\n",
        "10. What are the ACID properties in the context of MongoDB transactions\u001d\n",
        "\n",
        "  In MongoDB, ACID properties (Atomicity, Consistency, Isolation, and Durability) ensure the reliability and integrity of database transactions. They guarantee that a transaction is treated as a single, indivisible unit of work, with operations either completing successfully or not at all, and that the database remains in a consistent state throughout the process.\n",
        "\n",
        "11. What is the purpose of MongoDB’s explain() function\u001d\n",
        "\n",
        "  \n",
        "  explain() returns the queryPlanner information for the evaluated method. MongoDB runs the query optimizer to choose the winning plan, executes the winning plan to completion, and returns statistics describing the execution of the winning plan\n",
        "  \n",
        "\n",
        "12. How does MongoDB handle schema validation\u001d\n",
        "\n",
        "  MongoDB schema validation ensures that documents inserted or updated in a collection conform to a predefined structure and set of rules, enhancing data consistency and integrity. This is achieved by associating a JSON Schema with a collection, which specifies the required fields, data types, and other constraints for documents within that collection.\n",
        "\n",
        "13. What is the difference between a primary and a secondary node in a replica set\u001d\n",
        "\n",
        "  In a MongoDB replica set, the primary node handles all write operations and acts as the source of truth for data, while secondary nodes maintain copies of the data and can handle read operations, but cannot accept writes. If the primary node fails, a secondary node can be elected as the new primary to ensure continuous operation.\n",
        "\n",
        "14. What security mechanisms does MongoDB provide for data protection\u001d\n",
        "\n",
        "  MongoDB offers a comprehensive suite of security mechanisms for data protection, including encryption, authentication, authorization, and auditing. These features help protect data both in transit and at rest, and ensure that only authorized users can access and modify sensitive information.\n",
        "\n",
        "15. Explain the concept of embedded documents and when they should be used<\n",
        "\n",
        "  Embedded documents, also known as nested or sub-documents, are a way to store related data within a single document in a database like MongoDB. This means you can have a document within another document, creating a hierarchical structure. Embedded documents are useful when data is closely related and frequently accessed together, simplifying data retrieval and reducing the need for complex joins\n",
        "\n",
        "16. What is the purpose of MongoDB’s $lookup stage in aggregation\u001d\n",
        "\n",
        "  The $lookup stage in the Aggregation Framework is used to perform left outer joins with other collections. It allows you to combine documents from different collections based on a specified condition\n",
        "\n",
        "17. What are some common use cases for MongoDB\u001d\n",
        "\n",
        "  MongoDB is a versatile NoSQL database well-suited for a variety of use cases, particularly those involving large, diverse, and evolving datasets. It excels in scenarios requiring high performance, scalability, and flexibility in data modeling, such as content management, real-time analytics, and mobile applications. Its document-oriented structure also makes it a good fit for handling semi-structured and unstructured data commonly found in areas like IoT, social media, and gaming.\n",
        "\n",
        "18. What are the advantages of using MongoDB for horizontal scaling\u001d\n",
        "\n",
        "  MongoDB's sharding feature enables efficient horizontal scaling, offering several advantages: increased capacity, improved performance, enhanced fault tolerance, and cost-effectiveness. By distributing data across multiple servers (shards), MongoDB can handle large datasets and high traffic loads, preventing any single server from becoming a bottleneck.\n",
        "\n",
        "19.  How do MongoDB transactions differ from SQL transactions\u001d\n",
        "\n",
        "  SQL databases are used to store structured data while NoSQL databases like MongoDB are used to save unstructured data. MongoDB is used to save unstructured data in JSON format. MongoDB does not support advanced analytics and joins like SQL databases support.\n",
        "\n",
        "20. What are the main differences between capped collections and regular collections\u001d\n",
        "\n",
        "  Capped collections in MongoDB are fixed-size collections that automatically overwrite the oldest documents when the collection reaches its maximum size, while regular collections can grow dynamically and do not have this behavior. Capped collections also maintain insertion order and do not allow document deletion or modification.\n",
        "\n",
        "21. What is the purpose of the $match stage in MongoDB’s aggregation pipeline\u001d\n",
        "\n",
        "  A $match stage filters out a document from pipeline results if one of the following conditions applies: The $match query predicate returns a 0 , null , or false value on that document. The $match query predicate uses a field that is missing from that\n",
        "\n",
        "22. < How can you secure access to a MongoDB database\u001d\n",
        "\n",
        "  o secure a MongoDB database effectively, you should focus on enabling authentication and authorization, encrypting data both in transit and at rest, restricting network access, and implementing robust auditing and monitoring. These measures prevent unauthorized access, protect sensitive data, and help detect and respond to security breaches.\n",
        "\n",
        "23. What is MongoDB’s WiredTiger storage engine, and why is it important?\n",
        "\n",
        "  MongoDB's WiredTiger storage engine is a core component that manages how data is stored and accessed. It replaced the older MMAPv1 engine as the default in MongoDB 3.2 and is crucial for performance, scalability, and data integrity. WiredTiger's key features include document-level concurrency, checkpointing, compression, and encryption (for enterprise versions), making it suitable for a wide range of workloads.\n"
      ],
      "metadata": {
        "id": "SBWX-mH9Zggo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "                                                ##### Practical Questions"
      ],
      "metadata": {
        "id": "SagXpSQjnZTl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python script to load the Superstore dataset from a CSV file into MongoDB"
      ],
      "metadata": {
        "id": "fH9YKb09nclz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55200715"
      },
      "source": [
        "# Task\n",
        "Write a Python script to load the Superstore dataset from a CSV file into MongoDB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c130a6b"
      },
      "source": [
        "## Install libraries\n",
        "\n",
        "### Subtask:\n",
        "Install the `pymongo` and `pandas` libraries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f5ae38e"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires installing the `pymongo` and `pandas` libraries. I will use pip to install both libraries in a single cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a4090b0",
        "outputId": "5b9889b4-e9ce-4599-b354-05e9b9d67fba"
      },
      "source": [
        "%pip install pymongo pandas"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.11/dist-packages (4.13.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pymongo) (2.7.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6248a466"
      },
      "source": [
        "## Import libraries\n",
        "\n",
        "### Subtask:\n",
        "Import the necessary libraries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f187653"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary libraries for interacting with MongoDB and handling data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88ef19ec"
      },
      "source": [
        "import pandas as pd\n",
        "from pymongo import MongoClient"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "402f994f"
      },
      "source": [
        "## Load data\n",
        "\n",
        "### Subtask:\n",
        "Load the Superstore dataset from a CSV file into a pandas DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d58a3030"
      },
      "source": [
        "**Reasoning**:\n",
        "The current subtask is to load the data from the CSV file into a pandas DataFrame. The `pd.read_csv()` function is suitable for this purpose. After loading, displaying the head and info of the DataFrame is necessary to inspect the data structure and types as required by the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "e268b68d",
        "outputId": "d00703ca-336e-494d-c1c5-8149d316f182"
      },
      "source": [
        "df_superstore = pd.read_csv('Superstore.csv')\n",
        "display(df_superstore.head())\n",
        "display(df_superstore.info())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Superstore.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-2873181718.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_superstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Superstore.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_superstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_superstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Superstore.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4858725f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the file 'Superstore.csv' was not found. I need to check the available files in the current directory to confirm the file name and its location.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8-Dlajkn4ez",
        "outputId": "1a76945a-5cd8-4f4f-de92-b413172a8e55"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4360e274"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command showed that 'Superstore.csv' is not in the current directory. I will assume the file is located in the 'sample_data' directory based on the output of the `%ls` command. I will try to read the CSV file from the 'sample_data' directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "16f52216",
        "outputId": "e4921d28-dd54-4f6d-a952-e01d20e273ab"
      },
      "source": [
        "df_superstore = pd.read_csv('sample_data/Superstore.csv')\n",
        "display(df_superstore.head())\n",
        "display(df_superstore.info())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'sample_data/Superstore.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-1120525362.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_superstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample_data/Superstore.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_superstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_superstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample_data/Superstore.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a56a8502"
      },
      "source": [
        "**Reasoning**:\n",
        "The 'Superstore.csv' file was not found in the current directory or the 'sample_data' directory. I need to explore the contents of the 'sample_data' directory to see if the file exists there with a different name or if there are other files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S8c2Tuzn8aP",
        "outputId": "ea083764-762e-4c9f-963b-ac9bf2bb8c10"
      },
      "source": [
        "%ls sample_data"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;32manscombe.json\u001b[0m*                mnist_test.csv\n",
            "california_housing_test.csv   mnist_train_small.csv\n",
            "california_housing_train.csv  \u001b[01;32mREADME.md\u001b[0m*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f05bd0f"
      },
      "source": [
        "## Load data\n",
        "\n",
        "### Subtask:\n",
        "Load the Superstore dataset from a CSV file into a pandas DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "909b9c57"
      },
      "source": [
        "**Reasoning**:\n",
        "Since the previous attempt to locate the 'Superstore.csv' file failed, I will try another common location for sample datasets in environments like this, which is often a dedicated datasets directory. I will list the files in the root directory and a potential 'datasets' directory to locate the file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b5bc425",
        "outputId": "b39a5d82-ae3e-4256-87aa-2ac170ba1421"
      },
      "source": [
        "%ls /\n",
        "%ls /datasets"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;36mbin\u001b[0m@                        \u001b[01;36mlib32\u001b[0m@                    \u001b[01;34mroot\u001b[0m/\n",
            "\u001b[01;34mboot\u001b[0m/                       \u001b[01;36mlib64\u001b[0m@                    \u001b[01;34mrun\u001b[0m/\n",
            "\u001b[01;34mcontent\u001b[0m/                    \u001b[01;36mlibx32\u001b[0m@                   \u001b[01;36msbin\u001b[0m@\n",
            "cuda-keyring_1.1-1_all.deb  \u001b[01;34mmedia\u001b[0m/                    \u001b[01;34msrv\u001b[0m/\n",
            "\u001b[01;34mdatalab\u001b[0m/                    \u001b[01;34mmnt\u001b[0m/                      \u001b[01;34msys\u001b[0m/\n",
            "\u001b[01;34mdev\u001b[0m/                        NGC-DL-CONTAINER-LICENSE  \u001b[30;42mtmp\u001b[0m/\n",
            "\u001b[01;34metc\u001b[0m/                        \u001b[01;34mopt\u001b[0m/                      \u001b[01;34mtools\u001b[0m/\n",
            "\u001b[01;34mhome\u001b[0m/                       \u001b[01;34mproc\u001b[0m/                     \u001b[01;34musr\u001b[0m/\n",
            "\u001b[01;34mkaggle\u001b[0m/                     \u001b[01;34mpython-apt\u001b[0m/               \u001b[01;34mvar\u001b[0m/\n",
            "\u001b[01;36mlib\u001b[0m@                        \u001b[01;32mpython-apt.tar.xz\u001b[0m*\n",
            "ls: cannot access '/datasets': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f24b94ac"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to find the file in `/datasets` failed. I will try listing the files in the `/content` directory, which is another common location for user-uploaded files or datasets in this environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68b5627f",
        "outputId": "4ca3bab3-af17-48b7-95d5-e76d91292b8b"
      },
      "source": [
        "%ls /content"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4624ca6"
      },
      "source": [
        "**Reasoning**:\n",
        "The 'Superstore.csv' file was not found in the root, `/datasets`, or `/content` directories. Since the `/content` directory contains a `sample_data` subdirectory which I have already checked, I will try listing the contents of the root directory again, looking for any directories that might contain datasets. I will then check the contents of any promising directories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2."
      ],
      "metadata": {
        "id": "vgJrko3NoH5y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc8d4986"
      },
      "source": [
        "## Retrieve data\n",
        "\n",
        "### Subtask:\n",
        "Retrieve and print all documents from the Orders collection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "479a72b4"
      },
      "source": [
        "**Reasoning**:\n",
        "The current subtask is to retrieve and print all documents from the 'Orders' collection. I will use the `find()` method on the collection to get a cursor of all documents. Then I will iterate through the cursor and print each document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b14489b1"
      },
      "source": [
        "# Access the \"superstore\" database and the \"Orders\" collection\n",
        "db = client[\"superstore\"]\n",
        "collection = db[\"Orders\"]\n",
        "\n",
        "# Retrieve all documents from the collection\n",
        "for document in collection.find():\n",
        "    display(document)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Count and display the total number of documents in the Orders collection<\n"
      ],
      "metadata": {
        "id": "HdGFZ-AaoQQF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68d64597"
      },
      "source": [
        "## Count documents\n",
        "\n",
        "### Subtask:\n",
        "Count and display the total number of documents in the Orders collection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e589531d"
      },
      "source": [
        "**Reasoning**:\n",
        "To count the documents in the 'Orders' collection, I will use the `count_documents()` method. This method efficiently returns the number of documents that match a given query (in this case, an empty query `{}` to count all documents). I will then display the count."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6637d879"
      },
      "source": [
        "# Count the number of documents in the collection\n",
        "count = collection.count_documents({})\n",
        "\n",
        "# Display the count\n",
        "print(f\"Total number of documents in the Orders collection: {count}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a query to fetch all orders from the \"West\" region<"
      ],
      "metadata": {
        "id": "0SfpKau2ojh4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f209c5e"
      },
      "source": [
        "## Fetch orders from \"West\" region\n",
        "\n",
        "### Subtask:\n",
        "Write a query to fetch all orders from the \"West\" region."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3b1d7e0"
      },
      "source": [
        "**Reasoning**:\n",
        "To fetch documents where the 'Region' field is 'West', I will use the `find()` method with a query document specifying the condition `{\"Region\": \"West\"}`. I will then iterate through the cursor and display each matching document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdbeb1d3"
      },
      "source": [
        "# Fetch orders from the \"West\" region\n",
        "west_orders = collection.find({\"Region\": \"West\"})\n",
        "\n",
        "# Display the fetched orders\n",
        "for order in west_orders:\n",
        "    display(order)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a query to find orders where Sales is greater than 500<"
      ],
      "metadata": {
        "id": "SuR_F6Curr1W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "306e65e7"
      },
      "source": [
        "## Find orders with Sales greater than 500\n",
        "\n",
        "### Subtask:\n",
        "Write a query to find orders where Sales is greater than 500."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19a310ae"
      },
      "source": [
        "**Reasoning**:\n",
        "To find documents where the 'Sales' field is greater than 500, I will use the `find()` method with a query document using the `$gt` operator: `{\"Sales\": {\"$gt\": 500}}`. I will then iterate through the cursor and display each matching document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb6603e3"
      },
      "source": [
        "# Find orders where Sales is greater than 500\n",
        "high_sales_orders = collection.find({\"Sales\": {\"$gt\": 500}})\n",
        "\n",
        "# Display the fetched orders\n",
        "for order in high_sales_orders:\n",
        "    display(order)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  Fetch the top 3 orders with the highest Profit<\n"
      ],
      "metadata": {
        "id": "AiGzFxHWr1-N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2d075d6"
      },
      "source": [
        "## Fetch top 3 orders by Profit\n",
        "\n",
        "### Subtask:\n",
        "Fetch the top 3 orders with the highest Profit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2892b344"
      },
      "source": [
        "**Reasoning**:\n",
        "To fetch the top 3 orders with the highest profit, I will use the `find()` method. I will sort the results by the 'Profit' field in descending order using `sort(\"Profit\", -1)` and limit the results to 3 using `limit(3)`. I will then iterate through the cursor and display each matching document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc8a8af8"
      },
      "source": [
        "# Fetch the top 3 orders with the highest Profit\n",
        "top_profit_orders = collection.find().sort(\"Profit\", -1).limit(3)\n",
        "\n",
        "# Display the fetched orders\n",
        "for order in top_profit_orders:\n",
        "    display(order)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  Update all orders with Ship Mode as \"First Class\" to \"Premium Class.O\n"
      ],
      "metadata": {
        "id": "a3X-rXItsHKu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d24ed48"
      },
      "source": [
        "## Update Ship Mode\n",
        "\n",
        "### Subtask:\n",
        "Update all orders with Ship Mode as \"First Class\" to \"Premium Class\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce6136b4"
      },
      "source": [
        "**Reasoning**:\n",
        "To update all documents where the 'Ship Mode' field is \"First Class\", I will use the `update_many()` method. The filter document will be `{\"Ship Mode\": \"First Class\"}` and the update document will use the `$set` operator to change the 'Ship Mode' to \"Premium Class\". I will then print the number of documents modified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "000cae2e"
      },
      "source": [
        "# Update orders with Ship Mode \"First Class\" to \"Premium Class\"\n",
        "update_result = collection.update_many(\n",
        "    {\"Ship Mode\": \"First Class\"},\n",
        "    {\"$set\": {\"Ship Mode\": \"Premium Class\"}}\n",
        ")\n",
        "\n",
        "# Print the number of documents modified\n",
        "print(f\"Number of documents modified: {update_result.modified_count}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  Delete all orders where Sales is less than 50<"
      ],
      "metadata": {
        "id": "rIjTT0bSsPe2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45c017ea"
      },
      "source": [
        "## Delete orders with Sales less than 50\n",
        "\n",
        "### Subtask:\n",
        "Delete all orders where Sales is less than 50."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f148788"
      },
      "source": [
        "**Reasoning**:\n",
        "To delete documents where the 'Sales' field is less than 50, I will use the `delete_many()` method with a query document using the `$lt` operator: `{\"Sales\": {\"$lt\": 50}}`. I will then print the number of documents deleted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16626e1a"
      },
      "source": [
        "# Delete orders where Sales is less than 50\n",
        "delete_result = collection.delete_many({\"Sales\": {\"$lt\": 50}})\n",
        "\n",
        "# Print the number of documents deleted\n",
        "print(f\"Number of documents deleted: {delete_result.deleted_count}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Use aggregation to group orders by Region and calculate total sales per region<"
      ],
      "metadata": {
        "id": "7BbN8FsjsWm3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9444b07b"
      },
      "source": [
        "## Aggregate sales by Region\n",
        "\n",
        "### Subtask:\n",
        "Use aggregation to group orders by Region and calculate total sales per region."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd36a1db"
      },
      "source": [
        "**Reasoning**:\n",
        "To group orders by 'Region' and calculate the total sales for each region using aggregation, I will use the `aggregate()` method. The pipeline will include a `$group` stage to group by 'Region' and a `$sum` accumulator to calculate the total 'Sales' for each group. I will then iterate through the results and display them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6e212b0"
      },
      "source": [
        "# Aggregate sales by Region\n",
        "pipeline = [\n",
        "    {\"$group\": {\"_id\": \"$Region\", \"total_sales\": {\"$sum\": \"$Sales\"}}}\n",
        "]\n",
        "\n",
        "sales_by_region = collection.aggregate(pipeline)\n",
        "\n",
        "# Display the results\n",
        "print(\"Total sales per region:\")\n",
        "for result in sales_by_region:\n",
        "    display(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  Fetch all distinct values for Ship Mode from the collection"
      ],
      "metadata": {
        "id": "0EDHUK4EsdRi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b7f3e04"
      },
      "source": [
        "## Fetch distinct Ship Modes\n",
        "\n",
        "### Subtask:\n",
        "Fetch all distinct values for Ship Mode from the collection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91db70f3"
      },
      "source": [
        "**Reasoning**:\n",
        "To fetch all distinct values for the 'Ship Mode' field, I will use the `distinct()` method on the collection, specifying the field name 'Ship Mode'. I will then print the list of distinct values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beb68eac"
      },
      "source": [
        "# Fetch all distinct values for Ship Mode\n",
        "distinct_ship_modes = collection.distinct(\"Ship Mode\")\n",
        "\n",
        "# Display the distinct values\n",
        "print(\"Distinct Ship Modes:\")\n",
        "for ship_mode in distinct_ship_modes:\n",
        "    print(ship_mode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Count the number of orders for each category."
      ],
      "metadata": {
        "id": "homPW1xesi59"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4233d452"
      },
      "source": [
        "## Count orders by Category\n",
        "\n",
        "### Subtask:\n",
        "Count the number of orders for each category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a112a1f"
      },
      "source": [
        "**Reasoning**:\n",
        "To count the number of orders for each category using aggregation, I will use the `aggregate()` method. The pipeline will include a `$group` stage to group by 'Category' and a `$sum` accumulator set to 1 (`{\"$sum\": 1}`) to count the documents in each group. I will then iterate through the results and display them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e07cbdc2"
      },
      "source": [
        "# Count orders by Category\n",
        "pipeline = [\n",
        "    {\"$group\": {\"_id\": \"$Category\", \"count\": {\"$sum\": 1}}}\n",
        "]\n",
        "\n",
        "orders_by_category = collection.aggregate(pipeline)\n",
        "\n",
        "# Display the results\n",
        "print(\"Number of orders per category:\")\n",
        "for result in orders_by_category:\n",
        "    display(result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}